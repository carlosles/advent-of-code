{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 10: Smoke Basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import Counter, deque\n",
    "from functools import reduce\n",
    "\n",
    "from aoc2021.util import read_as_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{{<{{{{([{[([[()<>]{<>{}}]<([]())(()<>)>)((({}())[()[]])<<[][]>[{}[]]>)]{{(<{}<>>{<><>}]([<>[]]<',\n",
       " '[(<{{[{(<({{<<[]()><<>{}>>([<>[]]{<><>})}})>)}]}}>[{(<{({[{[[({}())((){})]({{}[]})]<<[<>{}]([][])>({<>()}',\n",
       " '(({<{[{({(([[([]())({}())]]({[[]{}]([][]))<((){})<{}<>>>))[(([<>[]]<[]>)(([]{}){{}{}}))])})[({<[{',\n",
       " '([{{[([<({<<<([]())[()[]]>{<()[]>[[]()]}>[{<[]{}><[]>>{<<>()>{[]()}}]>[[[[[]{}]([]<>)]<{<>{}}',\n",
       " '[[((<({<(<{<<{{}()}{[][]}>[((){})]>}>{((<({}<>)<{}()>>[[<>()]])<<<[][]><<>[]>>{<{}[]>(<>())}>)<{[[{']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test data.\n",
    "tdata = [\n",
    "    '[({(<(())[]>[[{[]{<()<>>',\n",
    "    '[(()[<>])]({[<{<<[]>>(',\n",
    "    '{([(<{}[<>[]}>{[]{[(<()>',\n",
    "    '(((({<>}<{<{<>}{[]{[]{}',\n",
    "    '[[<[([]))<([[{}[[()]]]',\n",
    "    '[{[{({}]{}}([{[{{{}}([]',\n",
    "    '{<[[]]>}<{[{[{[]{()[[[]',\n",
    "    '[<(<(<(<{}))><([]([]()',\n",
    "    '<{([([[(<>()){}]>(<<{{',\n",
    "    '<{([{{}}[<[[[<>{}]]]>[]]',\n",
    "]\n",
    "\n",
    "# Input data.\n",
    "data = read_as_list(Path('./day10-input.txt'), func=str.rstrip)\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puzzle answers\n",
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Input = list[list[str]]\n",
    "\n",
    "\n",
    "def matching(c: str) -> str:\n",
    "    match c:\n",
    "        case '(': return ')'\n",
    "        case ')': return '('\n",
    "        case '[': return ']'\n",
    "        case ']': return '['\n",
    "        case '{': return '}'\n",
    "        case '}': return '{'\n",
    "        case '<': return '>'\n",
    "        case '>': return '<'\n",
    "        case _: raise Exception('invalid character {c}')\n",
    "\n",
    "\n",
    "def parse_line(line: str) -> list[str]:\n",
    "    \"\"\"Return ([<first illegal character>], <stack remaining>).\"\"\"\n",
    "    openings = {'(', '[', '{', '<'}\n",
    "    closings = {')', ']', '}', '>'}\n",
    "    stack = deque()\n",
    "    for c in line:\n",
    "        if c in openings:\n",
    "            stack.append(c)\n",
    "        elif c in closings:\n",
    "            if matching(c) != stack.pop():\n",
    "                return [c], ''.join(stack)\n",
    "        else:\n",
    "            raise Exception('invalid character {c}')\n",
    "    return [], ''.join(stack)\n",
    "\n",
    "\n",
    "def corrupt_errors(data: Input) -> list[str]:\n",
    "    return [c for cs,_ in map(parse_line, data) for c in cs]\n",
    "\n",
    "\n",
    "def corrupt_errscore(errs: list[str]) -> int:\n",
    "    points = {')': 3, ']': 57, '}': 1197, '>': 25137}\n",
    "    return sum(cnt*points[e] for e,cnt in Counter(errs).items())\n",
    "\n",
    "\n",
    "assert parse_line('[(()[<>])]({[<{<<[]>>(')[0] == []\n",
    "assert parse_line('{([(<{}[<>[]}>{[]{[(<()>')[0] == ['}']\n",
    "assert len(corrupt_errors(tdata)) == 5\n",
    "assert corrupt_errors(tdata) == ['}',')',']',')','>']\n",
    "assert corrupt_errscore(corrupt_errors(tdata)) == 26397"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total syntax error score for corruption errors: 362271\n"
     ]
    }
   ],
   "source": [
    "n = corrupt_errscore(corrupt_errors(data))\n",
    "print(f'The total syntax error score for corruption errors: {n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocomplete(stack: str) -> str:\n",
    "    return ''.join(map(matching, reversed(stack)))\n",
    "\n",
    "\n",
    "def points(c: str) -> int:\n",
    "    match c:\n",
    "        case ')': return 1\n",
    "        case ']': return 2\n",
    "        case '}': return 3\n",
    "        case '>': return 4\n",
    "        case _: raise Exception('invalid character {c}')\n",
    "\n",
    "\n",
    "def autocomplete_score(css: list[str]) -> int:\n",
    "    return reduce(lambda x,y: 5*x + y, map(points, css), 0)\n",
    "\n",
    "\n",
    "def autocompletions(data: Input) -> list[str]:\n",
    "    return [autocomplete(st) for cs,st in map(parse_line, data) if not cs]\n",
    "\n",
    "\n",
    "def median(xs: list[int]) -> int:\n",
    "    return sorted(xs)[len(xs) // 2]\n",
    "\n",
    "\n",
    "def mid_autocomp_score(data: Input) -> int:\n",
    "    return median(list(map(autocomplete_score, autocompletions(data))))\n",
    "\n",
    "\n",
    "assert parse_line('[{}<([])()>]')[1] == ''\n",
    "assert autocomplete(parse_line('[({(<(())[]>[[{[]{<()<>>')[1]) == '}}]])})]'\n",
    "assert len(autocompletions(tdata)) == 5\n",
    "assert autocomplete_score(')}>]})') == 5566\n",
    "assert median([5,7,0,2,1]) == 2\n",
    "assert mid_autocomp_score(tdata) == 288957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The middle autocompletion score: 1698395182\n"
     ]
    }
   ],
   "source": [
    "n = mid_autocomp_score(data)\n",
    "print(f'The middle autocompletion score: {n}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34bf0df1641f6a327048e3b88c696c92118af801ec77f897a0ad57ee3c48bebd"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('venv_py310': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
